{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Parse the document to extract the data in the XML's < raw > tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from KafNafParserPy import KafNafParser\n",
    "\n",
    "# Directory containing the NAF files\n",
    "directory = './WES-Dataset/docs/'\n",
    "documents = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# Loop over all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    # Check if the file is a NAF file\n",
    "    if filename.endswith('.naf'):\n",
    "        # Full path to the NAF file\n",
    "        filepath = os.path.join(directory, filename)\n",
    "\n",
    "        # Open and parse the NAF file\n",
    "        with open(filepath, 'r') as file:\n",
    "            naf_obj = KafNafParser(file)\n",
    "\n",
    "            # find the <raw> tag and extract its text content\n",
    "            raw_data = naf_obj.get_raw()\n",
    "            documents[filename] = raw_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Tokenise the documentsâ€™ content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/malcolmborg/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "for doc in documents:\n",
    "    raw_data = documents[doc]\n",
    "    if(raw_data is not None):\n",
    "        tokens = nltk.word_tokenize(raw_data)\n",
    "        documents[doc] = tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Perform case-folding, stop-word removal and stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/malcolmborg/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def stem(token):\n",
    "    if token.endswith('s') or token.endswith('es'):\n",
    "        return token[:-1]\n",
    "    elif token.endswith('ed'):\n",
    "        return token[:-2]\n",
    "    elif token.endswith('ing'):\n",
    "        return token[:-3]\n",
    "    return token\n",
    "for doc in documents:\n",
    "    tokens = documents[doc]    \n",
    "    if(tokens is not None and len(tokens) > 0):\n",
    "        processed_tokens = [stem(token.lower()) for token in tokens if token.lower() not in stopwords]\n",
    "        documents[doc] = processed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Build the term by document matrix containing the T F.IDF weight\n",
    "for each term within each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "term_frequencies = defaultdict(lambda: defaultdict(int))\n",
    "document_frequencies = defaultdict(int)\n",
    "\n",
    "for doc in documents:\n",
    "    tokens = documents[doc]\n",
    "    for token in tokens:\n",
    "        term_frequencies[doc][token] += 1\n",
    "        document_frequencies[token] += 1\n",
    "\n",
    "tfidf_matrix = defaultdict(lambda: defaultdict(float))\n",
    "for document in term_frequencies:\n",
    "    for token in term_frequencies[document]:\n",
    "        tf = term_frequencies[document][token]\n",
    "        df = document_frequencies[token]\n",
    "        tfidf_matrix[document][token] = tf * math.log(len(documents) / df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
